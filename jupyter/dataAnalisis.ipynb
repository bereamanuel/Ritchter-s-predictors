{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nepal earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Manuel Berea Arellano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 0: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Drivendata] competition: \n",
    "\n",
    "Richter's Predictor: Modeling Earthquake Damage  https://www.drivendata.org/competitions/57/nepal-earthquake/page/134/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        DATA\n",
    "# ==================== #\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "#      PLOTING\n",
    "# ============================== #\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "#      CLUSTERING\n",
    "# ============================== #\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "#      PRE PROCESSING\n",
    "# ============================== #\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#      METRICS\n",
    "# ============================== #\n",
    "from sklearn.metrics import f1_score   # average='micro'\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#      MODEL SELECTION\n",
    "# ============================== #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "#      CLASIFICATORS\n",
    "# ============================== #\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#      OTHERS\n",
    "# ============================== #\n",
    "from sklearn.inspection import permutation_importance\n",
    "import multiprocessing\n",
    "\n",
    "#      WARNINGS\n",
    "# ============================== #\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump_session('notebook_env.db')\n",
    "#dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_reshape(img):\n",
    "    \"\"\"\n",
    "    For show a image.\n",
    "    Para mostrar una imagen.\n",
    "\n",
    "    Input\n",
    "    --------\n",
    "    img (String): path image\n",
    "\n",
    "    Output\n",
    "    --------\n",
    "    Image in the notebook\n",
    "\n",
    "    \"\"\"\n",
    "    img = Image.open('./images/'+img).convert('RGB')\n",
    "    img = img.resize((300,500))\n",
    "    img = np.asarray(img)\n",
    "    return img\n",
    "\n",
    "def img_reshape_more(img):\n",
    "    \"\"\"\n",
    "    For show a image.\n",
    "    Para mostrar una imagen.\n",
    "\n",
    "    Input\n",
    "    --------\n",
    "    img (String): path image\n",
    "\n",
    "    Output\n",
    "    --------\n",
    "    Image in the notebook\n",
    "\n",
    "    \"\"\"\n",
    "    img = Image.open('./images/'+img).convert('RGB')\n",
    "    img = img.resize((1000,500))\n",
    "    img = np.asarray(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities(df, test, n): \n",
    "    \"\"\"\n",
    "    Calculate probabilities for distinc geo level and target groups.\n",
    "    Calcular las probabilidades para los distintos geo level y grupos de la target.\n",
    "\n",
    "    Input\n",
    "    --------\n",
    "    df (DataFrame pandas)\n",
    "    n (Int): values 1,2,3 for distinct geo level\n",
    "\n",
    "    Output\n",
    "    --------\n",
    "    Dataframe with probabilities new columns\n",
    "    \n",
    "    \"\"\"\n",
    "    column = [f\"geo_level_{n}_id\"]\n",
    "    nom1 = [f\"prob1_geo{n}\"]\n",
    "    nom2 = [f\"prob2_geo{n}\"]\n",
    "    nom3 = [f\"prob3_geo{n}\"]\n",
    "    #This will save the probabilities in one column for each in df and dfOut\n",
    "    damage1 = dict()\n",
    "    damage2 = dict()\n",
    "    damage3 = dict()\n",
    "\n",
    "    for i, j in df[column].value_counts().iteritems():\n",
    "        n1 = len(df[df.damage_grade == 1][df[column[0]] == i])\n",
    "        n2 = len(df[df.damage_grade == 2][df[column[0]] == i])\n",
    "        n3 = len(df[df.damage_grade == 3][df[column[0]] == i])\n",
    "\n",
    "        damage1[i[0]] = n1/j\n",
    "        damage2[i[0]] = n2/j\n",
    "        damage3[i[0]] = n3/j\n",
    "\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    list3 = []\n",
    "\n",
    "    for i in df[column[0]]:\n",
    "        list1.append(damage1.get(i))\n",
    "        list2.append(damage2.get(i))\n",
    "        list3.append(damage3.get(i))\n",
    "\n",
    "    df[nom1[0]] = list1\n",
    "    df[nom2[0]] = list2\n",
    "    df[nom3[0]] = list3\n",
    "\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    list3 = []\n",
    "\n",
    "    for i in test[column[0]]:\n",
    "        list1.append(damage1.get(i))\n",
    "        list2.append(damage2.get(i))\n",
    "        list3.append(damage3.get(i))\n",
    "\n",
    "    test[nom1[0]] = list1\n",
    "    test[nom2[0]] = list2\n",
    "    test[nom3[0]] = list3\n",
    "    \n",
    "    return df , test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "A model is trained using  of the folds as training data;\n",
    "\n",
    "the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. \n",
    "\n",
    "Extratree classifier is a type of Random forest Classifier, the diference is the randomness goes one step further in the war splits are computed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the performance of our algorithms, we'll use the F1 score which balances the precision and recall of a classifier. Traditionally, the F1 score is used to evaluate performance on a binary classifier, but since we have three possible labels we will use a variant called the micro averaged F1 score. \n",
    "\n",
    "$$ F_{micro} = \\frac{ 2 \\cdot P_{micro} \\cdot R_{micro} }{P_{micro} + R_{micro}} $$\n",
    "\n",
    "where \n",
    "$$ P_{micro} = \\frac{\\sum TP_k}{\\sum (TP_k + FP_k)}, \\ with \\ k=1,2,3 $$\n",
    "and\n",
    "$$ R_{micro} = \\frac{\\sum TP_k}{\\sum (TP_k + FN_k)}, \\ with \\ k=1,2,3 $$\n",
    "\n",
    "$TP$ is true positive, $FP$ is False positive, $FN$ is False negative.\n",
    "\n",
    "In python, you can use f_score with average ='micro'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-bag error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring the prediction error of random forests, boosted decision trees, and other machine learning models utilizing bootstrap aggregating (bagging). Bagging uses subsampling with replacement to create training samples for the model to learn from. OOB error is the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset mainly consists of information on the buildings' structure and their legal ownership. Each row in the dataset represents a specific building in the region that was hit by Gorkha earthquake.\n",
    "\n",
    "*----*\n",
    "\n",
    "\n",
    "El conjunto de datos principalmente consiste en la información estuctural de los datos y su propietario legal. Cada fila del conjunto de datos representa un edificio concreto en la region donde tuvo lugar el terremoto Gorkha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train_values.csv\")\n",
    "trainTar = pd.read_csv(\"../data/train_labels.csv\")\n",
    "\n",
    "test = pd.read_csv(\"../data/test_values.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge on one dataframe because it'll permit a better analysis. For feature engineering, we'll merge test to.\n",
    "\n",
    "*----*\n",
    "\n",
    "Unimos en un solo dataframe ya que nos permite un mejor analisis de la target. Para las distintas transformaciones también tendremos que unir el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.merge(trainTar , on= \"building_id\" , how= \"left\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} rows and {} columns \\n  -Categorical: 8 \\n  -Binary: 22 \\n  -Integer: 6 \".format(len(df) , len(df.columns)))\n",
    "print(\"- - - - - - - - - - - - - - - - - - - -\")\n",
    "print(\"Tenemos {} filas y {} columns \\n  -Categoricas: 8 \\n  -Binarias: 22 \\n  -Enetero: 6 \".format(len(df) , len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} rows duplicated\".format(len(df) - df.duplicated().value_counts().values[0]))\n",
    "print(\"- - - - - - - - - - - - - - - - - - - -\")\n",
    "print(\"Hay {} filas duplicadas\".format(len(df) - df.duplicated().value_counts().values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} nan values\".format(df.isna().sum().sum()))\n",
    "print(\"- - - - - - - - - - - - - - - - - - - -\")\n",
    "print(\"Hay {} valores nulos\".format(df.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not nan values, but we should see if there are wrong values\n",
    "\n",
    "*-----*\n",
    "\n",
    "No hay valores nulos, pero deberíamos mirar si hay errores en algún valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We select only the int columns\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - -\")\n",
    "print(\"Seleccionamos solo las columnas con dato entero\")\n",
    "df[[\"count_floors_pre_eq\", \"age\" , \"area_percentage\" , \"height_percentage\", \"count_families\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 0 and 995 value for age column, this is a error value. 0 is it posible but 995 is strange.\n",
    "\n",
    "*-----*\n",
    "\n",
    "Vemos como hay valor 0 y 995 en la columna edad, eso es un valor erroneo. 0 es posible, pero 995 es un valor extraño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(data = df, y =\"age\" , x=\"damage_grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With boxplot we see that very well. If we won't use robust scaler, we should drop outliers. \n",
    "\n",
    "*-----*\n",
    "\n",
    "Con el boxplot lo podemos ver muy bien. Si no vamos a usar robust scaler, debemos borrar los outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data = df, x =\"age\" , kind= \"count\" ,  aspect=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(df[df['age']==995])} values with value 995, i.e {round( 100*len(df[df['age']==995]) / len(df) ,2)} % of total, these probably are unknown values.\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - -\")\n",
    "print(f\"Hay {len(df[df['age']==995])} valores con 995, i.e {round( 100*len(df[df['age']==995]) / len(df) ,2)} % the total, son probablemente valores desconocidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the distribution:\n",
    "\n",
    "*-----*\n",
    "\n",
    "Aquí podemos ver la distribución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['age']==995].groupby(['damage_grade']).count()['building_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to drop outliers, but in other hand we analize the age in groups and we'll see what is better.\n",
    "It seems that would nice make groups with the age, because there are a lot of outliers, and we can make groups to solve this problem.\n",
    "\n",
    "We'll do this in the next chapter.\n",
    "\n",
    "*----*\n",
    "\n",
    "Vamos a borrar los outliers, pero por otro lado vamos a analizar la edad en grupos y veremos cual es mejor.\n",
    "Parece optimo hacer grupos de edad de los edificiones, porque hay muchos outliers y podemos hacer grupos para resolver este problema.\n",
    "\n",
    "Heremos esto en el proximo capítulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_1 = df.describe().loc['25%'].loc['age']\n",
    "Q_3 = df.describe().loc['75%'].loc['age']\n",
    "IQR = Q_3 - Q_1\n",
    "df['outliers'] = df['age'].apply( lambda x : 1 if ((x < (Q_1 - 1.5*IQR) ) or ( (Q_3 + 1.5 * IQR) < x ) ) else 0 )\n",
    "print(f\"There are {df['outliers'].sum()} outliers, i.e {round( 100*df['outliers'].sum() / len(df) , 2 )} % of total, we are going to drop it\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - -\")\n",
    "print(f\"Hay {df['outliers'].sum()} outliers, es un {round( 100*df['outliers'].sum() / len(df) , 2 )} % del total, vamos a sacarlos del estudio\")\n",
    "dfOut = df[df['outliers'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(data = dfOut, y =\"age\" , x=\"damage_grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data = dfOut, x =\"age\" , kind= \"count\" ,  aspect=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOut[[\"count_floors_pre_eq\", \"age\" , \"area_percentage\" , \"height_percentage\", \"count_families\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to see \"area_percentage\" and \"height_percentage\", because it seems like there are outliers too.\n",
    "\n",
    "*-----*\n",
    "\n",
    "Vamos a ver la columna \"area_percentage\" y \"height_percentage\", porque parece que también tiene outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfOut[[\"area_percentage\", \"height_percentage\"]].describe())\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "fig.suptitle(\"Area and Height percentage boxplot\")\n",
    "#-----#\n",
    "ax1.set_title(\"Area percentage\")\n",
    "ax1.boxplot(dfOut[[\"area_percentage\"]])\n",
    "#-----#\n",
    "ax2.set_title(\"Hight percentage\")\n",
    "ax2.boxplot(dfOut[[\"height_percentage\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these columns, probably the best idea is use a robust scaler, because 100% value is posible and don't seems an unknown value.\n",
    "\n",
    "*-----*\n",
    "\n",
    "Para estas columns, probablemente la mejor idea es usar un robust scaler, porque el valor 100% es posible y no parece un valor desconocido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before outliers analysis we have:\n",
    "\n",
    "*------*\n",
    "\n",
    "Después del analisis de outliers tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We drop outlierns, now we have {round (100 - len(dfOut)*100 / len(df) ,2)} % less rows than the initial dataframe.\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - -\")\n",
    "print(f\"Hemos borrado outliers, ahora tenemos un {round (100 - len(dfOut)*100 / len(df) ,2)} % menos filas que el dataframe original.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = [\"count_floors_pre_eq\", \"age\" , \"area_percentage\" , \"height_percentage\", \"count_families\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrTot = df[numeric].corr()\n",
    "corrOut = dfOut[numeric].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the diferences between corrTot and corrOut\n",
    "\n",
    "*-----*\n",
    "\n",
    "Aqui vemos las diferencias entra ambas correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(corrTot-corrOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to analize only the corrTot, because the diferences are minimun.\n",
    "\n",
    "*------*\n",
    "\n",
    "Vamos a analizar la correlacion solo de corrTot, porque las diferencias son mínimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORPLOT\n",
    "plt.figure(figsize=(8, 9))\n",
    "maskTot = np.triu(np.ones_like(corrTot, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corrTot, mask=maskTot, cmap=cmap, vmax=.3, center=0, annot= True,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .8} )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest correlation is between number floors pre earthquake and the porcent height. But 0.77 is not enought for drop the column.\n",
    "\n",
    "*------*\n",
    "\n",
    "La correlación más alta es entre numero de plantas antes del terremoto y el porcentaje de altura. Pero 0.77 no es suficiente para borrar la columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Featuring Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['land_surface_condition','foundation_type','roof_type','ground_floor_type','other_floor_type','position','plan_configuration','legal_ownership_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Land surface condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surface condition of the land where the building was built. \n",
    "\n",
    "*-----*\n",
    "\n",
    "Estado de la superficio del terreno donde se contruyó el edificio.\n",
    "\n",
    "\n",
    "Possible values: n, o, t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[0]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "ax1.hist(data = df , x = categorical[0] , bins=len(set(df[categorical[0]])))\n",
    "ax1.set_title(\"Dataframe total\")\n",
    "ax2.hist(data = dfOut , x = categorical[0], bins=len(set(df[categorical[0]])))\n",
    "ax2.set_title(\"Dataframe without outliers\")\n",
    "plt.suptitle(categorical[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[0] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this to analisis, we are going to group the o and n values in one.\n",
    "\n",
    "*-----*\n",
    "\n",
    "Con este analisis, vamos a agrupar los valores o y n en solo uno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[0]] = df[categorical[0]].apply(lambda x: 1 if x == 't' else 0)\n",
    "dfOut[categorical[0]] = dfOut[categorical[0]].apply(lambda x: 1 if x == 't' else 0)\n",
    "\n",
    "test[categorical[0]] = test[categorical[0]].apply(lambda x: 1 if x == 't' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[0] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Foundation type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of foundation used while building. \n",
    "\n",
    "*----*\n",
    "\n",
    "Tipo de financiacion usada cuando se contruyó el edificio.\n",
    "\n",
    "Possible values: h, i, r, u, w.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[1]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "ax1.hist(data = df , x = categorical[1] , bins=len(set(df[categorical[1]])))\n",
    "ax1.set_title(\"Dataframe total\")\n",
    "ax2.hist(data = dfOut , x = categorical[1], bins=len(set(df[categorical[1]])))\n",
    "ax2.set_title(\"Dataframe without outliers\")\n",
    "plt.suptitle(categorical[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[1] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[1]] = df[categorical[1]].apply(lambda x : 1 if x == 'r' else 0)\n",
    "dfOut[categorical[1]] = dfOut[categorical[1]].apply(lambda x : 1 if x == 'r' else 0)\n",
    "\n",
    "test[categorical[1]] = test[categorical[1]].apply(lambda x : 1 if x == 'r' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[1] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Roof type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of roof used while building. \n",
    "\n",
    "*-----*\n",
    "\n",
    "Tipo de tejado empleado al construir el edificio\n",
    "\n",
    "Possible values: n, q, x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[2]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "ax1.hist(data = df , x = categorical[2] , bins=len(set(df[categorical[2]])))\n",
    "ax1.set_title(\"Dataframe total\")\n",
    "ax2.hist(data = dfOut , x = categorical[2], bins=len(set(df[categorical[2]])))\n",
    "ax2.set_title(\"Dataframe without outliers\")\n",
    "plt.suptitle(categorical[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[2] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[2]] = df[categorical[2]].apply(lambda x : 1 if x == 'q' else 0)\n",
    "dfOut[categorical[2]] = dfOut[categorical[2]].apply(lambda x : 1 if x == 'q' else 0)\n",
    "\n",
    "test[categorical[2]] = test[categorical[2]].apply(lambda x : 1 if x == 'q' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[2] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ground floor type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of the ground floor. \n",
    "\n",
    "*-----*\n",
    "\n",
    "Tipo de suelo.\n",
    "\n",
    "Possible values: f, m, v, x, z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[3]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "ax1.hist(data = df , x = categorical[3] , bins=len(set(df[categorical[3]])))\n",
    "ax1.set_title(\"Dataframe total\")\n",
    "ax2.hist(data = dfOut , x = categorical[3], bins=len(set(df[categorical[3]])))\n",
    "ax2.set_title(\"Dataframe without outliers\")\n",
    "plt.suptitle(categorical[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[3]] = df[categorical[3]].apply(lambda x : 'other' if x == 'z' else 'other' if x == 'm' else x)\n",
    "dfOut[categorical[3]] = dfOut[categorical[3]].apply(lambda x : 'other' if x == 'z' else 'other' if x == 'm' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[3]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "sns.catplot(data= df , x = categorical[3] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[3]] = df[categorical[3]].apply(lambda x : 1 if x == 'f' else 0)\n",
    "dfOut[categorical[3]] = dfOut[categorical[3]].apply(lambda x : 1 if x == 'f' else 0)\n",
    "\n",
    "test[categorical[3]] = test[categorical[3]].apply(lambda x : 1 if x == 'f' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[3] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other floor type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Type of constructions used in higher than the ground floors (except of roof). \n",
    " \n",
    " *-----*\n",
    "\n",
    " Tipo de materiales usados en plantas superiores distintas a la baja y techo.\n",
    " \n",
    " Possible values: j, q, s, x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[4]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "ax1.hist(data = df , x = categorical[4] , bins=len(set(df[categorical[4]])))\n",
    "ax1.set_title(\"Dataframe total\")\n",
    "ax2.hist(data = dfOut , x = categorical[4], bins=len(set(df[categorical[4]])))\n",
    "ax2.set_title(\"Dataframe without outliers\")\n",
    "plt.suptitle(categorical[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[4] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[4]] = df[categorical[4]].apply(lambda x : 'j' if x == 's' else x)\n",
    "dfOut[categorical[4]] = dfOut[categorical[4]].apply(lambda x : 'j' if x == 's' else x)\n",
    "\n",
    "test[categorical[4]] = test[categorical[4]].apply(lambda x : 'j' if x == 's' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[4]] = df[categorical[4]].apply(lambda x : 2 if x == 'q' else 1 if x == 'x' else  0)\n",
    "dfOut[categorical[4]] = dfOut[categorical[4]].apply(lambda x : 2 if x == 'q' else 1 if x == 'x' else  0)\n",
    "\n",
    "test[categorical[4]] = test[categorical[4]].apply(lambda x : 2 if x == 'q' else 1 if x == 'x' else  0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[4] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Position of the building. \n",
    "\n",
    "*-----*\n",
    "\n",
    "Posición del edificio.\n",
    "\n",
    "Possible values: j, o, s, t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[5]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "ax1.hist(data = df , x = categorical[5] , bins=len(set(df[categorical[5]])))\n",
    "ax1.set_title(\"Dataframe total\")\n",
    "ax2.hist(data = dfOut , x = categorical[5], bins=len(set(df[categorical[5]])))\n",
    "ax2.set_title(\"Dataframe without outliers\")\n",
    "plt.suptitle(categorical[5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[5] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[5]] = df[categorical[5]].apply(lambda x : 1 if x == 's' else 0)\n",
    "dfOut[categorical[5]] = dfOut[categorical[5]].apply(lambda x : 1 if x == 's' else 0)\n",
    "\n",
    "test[categorical[5]] = test[categorical[5]].apply(lambda x : 1 if x == 's' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[5] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plan configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building plan configuration. \n",
    "\n",
    "*-----*\n",
    "\n",
    "Configuración del plano del edificio.\n",
    "\n",
    "Possible values: a, c, d, f, m, n, o, q, s, u."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[6]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "ax1.hist(data = df , x = categorical[6] , bins=len(set(df[categorical[6]])))\n",
    "ax1.set_title(\"Dataframe total\")\n",
    "ax2.hist(data = dfOut , x = categorical[6], bins=len(set(df[categorical[6]])))\n",
    "ax2.set_title(\"Dataframe without outliers\")\n",
    "plt.suptitle(categorical[6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[6] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical[6]] = df[categorical[6]].apply(lambda x : 1 if x == 'd' else 0)\n",
    "dfOut[categorical[6]] = dfOut[categorical[6]].apply(lambda x : 1 if x == 'd' else 0)\n",
    "\n",
    "test[categorical[6]] = test[categorical[6]].apply(lambda x : 1 if x == 'd' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = categorical[6] , kind= 'count', hue = 'damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is not well clustered, we are going to drop it\n",
    "\n",
    "*----*\n",
    "\n",
    "Esta columna no está bien repartida, vamos a eliminarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(categorical[6], axis = 1)\n",
    "dfOut = dfOut.drop(categorical[6], axis = 1)\n",
    "\n",
    "test = test.drop(categorical[6], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Legal ownership status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legal ownership status of the land where building was built. \n",
    "\n",
    "*-----*\n",
    "\n",
    "Situación de la propiedad legal del terreno donde se construyó el edificio. \n",
    "\n",
    "\n",
    "Possible values: a, r, v, w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.groupby(categorical[7]).count()['building_id'] *100 / len(df) ,2 ) )\n",
    "fig , (ax1,ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "ax1.hist(data = df , x = categorical[7] , bins=len(set(df[categorical[7]])))\n",
    "ax1.set_title(\"Dataframe total\")\n",
    "ax2.hist(data = dfOut , x = categorical[7], bins=len(set(df[categorical[7]])))\n",
    "ax2.set_title(\"Dataframe without outliers\")\n",
    "plt.suptitle(categorical[7])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(categorical[7], axis = 1)\n",
    "dfOut = dfOut.drop(categorical[7], axis = 1)\n",
    "\n",
    "test = test.drop(categorical[7], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geographic region in which building exists, from largest (level 1) to most specific sub-region (level 3). \n",
    "\n",
    "\n",
    "*-----*\n",
    "\n",
    "\n",
    "Region geográfica en donde hay edificios, del más grande (level 1) al mas especifico, sub region (level 3).\n",
    "\n",
    "Possible values: level 1: 0-30, level 2: 0-1427, level 3: 0-12567."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geos = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,1,2]:\n",
    "    print(\"\\n- - - - - - - \")\n",
    "    with pd.option_context('display.max_rows',10):\n",
    "        print(df[geos[i]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate the probability of belonging to one target group or another based on the geo level columns. This will help us to cluster the diferent positions.\n",
    "\n",
    "*-------*\n",
    "\n",
    "Vamos a calcular la probabilidad de pertenecer a un grupo de la target u otro en función de las columnas de geo level. Esto nos ayudará a relacionar las distintas posiciones y clusterizarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df, test = probabilities(df,test,1)\n",
    "#df, test = probabilities(df,test,2)\n",
    "#df, test = probabilities(df,test,3)\n",
    "\n",
    "#df.to_csv(\"../data/trainProb.csv\")\n",
    "#test.to_csv(\"../data/testProb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfOut, testOut = probabilities(dfOut,test,1)\n",
    "#dfOut, testOut = probabilities(dfOut,test,2)\n",
    "#dfOut, testOut = probabilities(dfOut,test,3)\n",
    "\n",
    "#dfOut.to_csv(\"../data/trainOutProb.csv\")\n",
    "#testOut.to_csv(\"../data/testOutProb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"../data/trainProb.csv\")\n",
    "#test = pd.read_csv(\"../data/testProb.csv\")\n",
    "\n",
    "#dfOut = pd.read_csv(\"../data/trainOutProb.csv\")\n",
    "#testOut = pd.read_csv(\"../data/testOutProb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = ['prob1_geo1','prob2_geo1', 'prob3_geo1', 'prob1_geo2', 'prob2_geo2', 'prob3_geo2','prob1_geo3', 'prob2_geo3', 'prob3_geo3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nClus = range(3, 10)\n",
    "kmeans = [KMeans(n_clusters=i, random_state= 1995) for i in nClus]\n",
    "\n",
    "score = [kmeans[i].fit(df[probs]).score(df[probs]) for i in range(len(kmeans))]\n",
    "\n",
    "plt.plot(nClus,score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seen the plot, I'll choose 5 clusters.\n",
    "\n",
    "*-------*\n",
    "\n",
    "Viendo el gráfico, voy a escoger 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5 , random_state = 1995 )\n",
    "kmeans.fit(df[probs])\n",
    "\n",
    "# Predicting the clusters\n",
    "labels = kmeans.predict(df[probs])\n",
    "labelsT = kmeans.predict(test[probs])\n",
    "\n",
    "df['clusters'] = labels\n",
    "test['clusters'] = labelsT\n",
    "\n",
    "# *-----*\n",
    "\n",
    "kmeans = KMeans(n_clusters = 5 , random_state = 1995 )\n",
    "kmeans.fit(dfOut[probs])\n",
    "\n",
    "# Predicting the clusters\n",
    "labels = kmeans.predict(dfOut[probs])\n",
    "labelsT = kmeans.predict(testOut[probs])\n",
    "\n",
    "dfOut['clusters'] = labels\n",
    "testOut['clusters'] = labelsT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data= df , x = \"clusters\" , kind=\"count\" , hue= \"damage_grade\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to analize the binary columns.\n",
    "\n",
    "*-----*\n",
    "\n",
    "Finalmente vamos a analizar las columnas de valor binario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = ['has_secondary_use','has_secondary_use_agriculture','has_secondary_use_gov_office','has_secondary_use_health_post',\n",
    "            'has_secondary_use_hotel','has_secondary_use_industry','has_secondary_use_institution','has_secondary_use_other',\n",
    "            'has_secondary_use_rental','has_secondary_use_school','has_secondary_use_use_police','has_superstructure_adobe_mud',\n",
    "            'has_superstructure_bamboo','has_superstructure_cement_mortar_brick','has_superstructure_cement_mortar_stone', \n",
    "            'has_superstructure_mud_mortar_brick', 'has_superstructure_mud_mortar_stone', 'has_superstructure_other', \n",
    "            'has_superstructure_rc_engineered','has_superstructure_rc_non_engineered', 'has_superstructure_stone_flag', 'has_superstructure_timber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t\\t% of 1 in the columns\")\n",
    "print(\"\\t- - - - - - - - - - - - - - - - - - - \")\n",
    "print(\"\\t\\t% de 1 en las columnas\")\n",
    "round(df[binary].sum() *100 / len(df) , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = list()\n",
    "for x in binary:\n",
    "    if (round(df[x].sum() *100 / len(df) , 2) < 10.0) == True:\n",
    "        dp.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod = df.drop(dp, axis=1)\n",
    "dfOut_mod = dfOut.drop(dp, axis=1)\n",
    "test_mod = test.drop(dp, axis=1)\n",
    "testOut_mod = testOut.drop(dp, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first model, we'll use:\n",
    "\n",
    "*------*\n",
    "\n",
    "Para el primer modelo, vamos a usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = [\"count_floors_pre_eq\", \"age\" , \"area_percentage\" , \"height_percentage\", \"count_families\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count floors pre earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of floors in the building before the earthquake.\n",
    "\n",
    "*------*\n",
    "\n",
    "Numero de plantas del edificio pre terremoto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data = df , x = numeric[0] , kind= 'count', hue='damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric[0]] = df[numeric[0]].apply(lambda x: x if x<3 else 3)\n",
    "dfOut[numeric[0]] = dfOut[numeric[0]].apply(lambda x: x if x<3 else 3)\n",
    "test[numeric[0]] = test[numeric[0]].apply(lambda x: x if x<3 else 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data = df , x = numeric[0] , kind= 'count', hue='damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age of the building in years.\n",
    "\n",
    "*------*\n",
    "\n",
    "Edad del edificio en años."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data = dfOut , x = numeric[1] , kind= 'count', hue='damage_grade', height=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOut[numeric[1]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs robust scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized area and height of the building footprint.\n",
    "\n",
    "*------*\n",
    "\n",
    "Superficie y altura normalizadas de la huella del edificio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of families that live in the building.\n",
    "\n",
    "*-------*\n",
    "\n",
    "Numero de familias que viven en el edificio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data = df , x = numeric[4] , kind= 'count', hue='damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric[4]] = df[numeric[4]].apply(lambda x: x if x<2 else 2)\n",
    "dfOut[numeric[4]] = dfOut[numeric[4]].apply(lambda x: x if x<2 else 2)\n",
    "test[numeric[4]] = test[numeric[4]].apply(lambda x: x if x<2 else 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data = df , x = numeric[4] , kind= 'count', hue='damage_grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First models: Evaluating and comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'damage_grade'\n",
    "numeric = [ \"age\" ,'prob1_geo1','prob1_geo2','prob1_geo3','prob2_geo1','prob2_geo2','prob2_geo3','prob3_geo1','prob3_geo2','prob3_geo3']\n",
    "categorical = ['clusters','roof_type',\"area_percentage\" , \"height_percentage\",\"count_floors_pre_eq\", \"count_families\",'foundation_type','ground_floor_type','has_secondary_use','has_superstructure_mud_mortar_stone','has_superstructure_timber','land_surface_condition','other_floor_type','position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_mod.columns) - set([target]) - set(numeric) - set(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "escaler = RobustScaler().fit(df[numeric])\n",
    "escalerOut = RobustScaler().fit(dfOut[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric] = escaler.transform(df[numeric])\n",
    "test[numeric] = escaler.transform(test[numeric])\n",
    "\n",
    "dfOut[numeric] = escalerOut.transform(dfOut[numeric])\n",
    "testOut[numeric] = escalerOut.transform(testOut[numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = [target,'outliers']), df[target],random_state = 1995)\n",
    "X_trainO, X_testO, y_trainO, y_testO = train_test_split(dfOut.drop(columns = [target,'outliers']), dfOut[target],random_state = 1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: ExtratreeClasifier without hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = ExtraTreesClassifier(\n",
    "            n_estimators = 150,\n",
    "            criterion    = 'gini',\n",
    "            max_depth    = 10,\n",
    "            max_features = 'auto',\n",
    "            n_jobs       = -1,\n",
    "            random_state = 1995)\n",
    "\n",
    "\n",
    "modeloOut = ExtraTreesClassifier(\n",
    "            n_estimators = 150,\n",
    "            criterion    = 'gini',\n",
    "            max_depth    = 10,\n",
    "            max_features = 'auto',\n",
    "            n_jobs       = -1,\n",
    "            random_state = 1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.fit(X_train, y_train) , modeloOut.fit(X_trainO, y_trainO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo inicial\n",
    "# ==============================================================================\n",
    "predicciones = modelo.predict(X = X_test)\n",
    "\n",
    "f1score1 = f1_score(\n",
    "        y_true  = y_test,\n",
    "        y_pred  = predicciones,\n",
    "        average = 'micro'\n",
    "       )\n",
    "print(f\"El f1 score de test es: {f1score1} con outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = modelo.predict(test)\n",
    "test['damage_grade'] = predicciones\n",
    "test[['building_id','damage_grade']].to_csv(\"../predictions/predETCmodel1.csv\", index = False)\n",
    "\n",
    "\n",
    "prediccionesOut = modeloOut.predict(testOut)\n",
    "testOut['damage_grade'] = prediccionesOut\n",
    "testOut[['building_id','damage_grade']].to_csv(\"../predictions/predOutETCmodel1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a first point to improve. The next steps are:\n",
    "    \n",
    "    - Tunning the model\n",
    "  \n",
    "    - Compare with other algorithms\n",
    "  \n",
    "    - Improve our featuring engineering, for exmple, changing the clustering.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model with outliers\")\n",
    "pred_model1 = img_reshape_more('/prediccions/one.jpg')\n",
    "plt.imshow(np.asarray(pred_model1))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model without outliers\")\n",
    "pred_model1 = img_reshape_more('/prediccions/oneOut.jpg')\n",
    "plt.imshow(np.asarray(pred_model1))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: ExtratreeClasifier with hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Num estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a loop for training the diferent models with each n_estimador value and take train error and Out-of Bag value. It'll permit us choose the best parameter option for our dataframe.\n",
    "It's important choose the less number of trees with the best score, more trees wont improve our power prediction but it will make us don't have the best performance.\n",
    "\n",
    "*------*\n",
    "\n",
    "Vamos a utilizar un bucle para entrenar los diferentes moderlos con cada uno de los valores de los estimadores y cogeremos el error y el valor Out.of-bag. Esto nos va a permitir elegir la mejor opcion de parámetros para nuestro conjunto de datos. Es importante elegir el menor número de árboles con el mayor score, más arboles no implica mayor poder predictivo, pero si que hará empeorar nuestro performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "oob_scores   = []\n",
    "\n",
    "estimator_range = range(1, 200, 25)\n",
    "\n",
    "# Loop for training models with each n_estimator value and take\n",
    "# train error and Out-of-Bag value.\n",
    "for n_estimators in estimator_range:\n",
    "    model = ExtraTreesClassifier(\n",
    "                n_estimators = n_estimators,\n",
    "                criterion    = 'gini',\n",
    "                max_depth    = 10,\n",
    "                max_features = 'auto',\n",
    "                bootstrap    = True,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 1995\n",
    "             )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    train_scores.append(model.score(X_train, y_train))\n",
    "    oob_scores.append(model.oob_score_)\n",
    "    \n",
    "# Graph for analize the error\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "ax.plot(estimator_range, oob_scores, label=\"out-of-bag scores\")\n",
    "ax.plot(estimator_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "        marker='o', color = \"red\", label=\"max score\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_title(\"Evolution out-of-bag-error vs num estimators\")\n",
    "plt.legend();\n",
    "print(f\"Best value of n_estimators: {estimator_range[np.argmax(oob_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use cross validation with f1 score. \n",
    "\n",
    "*-----*\n",
    "\n",
    "Ahora vamos a utilizar cross validation con la métrica f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV validation\n",
    "# ==============================================================================\n",
    "estimator_range = range(1, 100, 20)\n",
    "cv_scores = []\n",
    "for n_estimators in estimator_range:\n",
    "    \n",
    "    modelo = ExtraTreesClassifier(\n",
    "                n_estimators = n_estimators,\n",
    "                criterion    = 'gini',\n",
    "                max_depth    = 10,\n",
    "                max_features = 'auto',\n",
    "                bootstrap    = True,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 1995\n",
    "             )\n",
    "    scores = cross_val_score(\n",
    "                estimator = modelo,\n",
    "                X         = X_train,\n",
    "                y         = y_train,\n",
    "                scoring   = 'f1_micro',\n",
    "                cv        = 5\n",
    "             )   \n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "print(f\"Best value of n_estimators: {estimator_range[np.argmax(cv_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should take around 50 and 60 trees.\n",
    "\n",
    "*-----*\n",
    "\n",
    "Debemos coger entre 50 y 60 árboles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Max features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV validation\n",
    "# ==============================================================================\n",
    "max_features_range = range(1, X_train.shape[1] + 1, 1)\n",
    "cv_scores = []\n",
    "for max_features in max_features_range:\n",
    "    \n",
    "    modelo = ExtraTreesClassifier(\n",
    "                n_estimators = 55,\n",
    "                criterion    = 'gini',\n",
    "                max_depth    = 10,\n",
    "                max_features = max_features,\n",
    "                bootstrap    = True,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 1995\n",
    "             )\n",
    "    scores = cross_val_score(\n",
    "                estimator = modelo,\n",
    "                X         = X_train,\n",
    "                y         = y_train,\n",
    "                scoring   = 'f1_micro',\n",
    "                cv        = 5\n",
    "             )   \n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "    \n",
    "print(f\"Best value of n_estimators: {max_features_range[np.argmax(cv_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid de hiperparámetros evaluados\n",
    "# ==============================================================================\n",
    "param_grid = {'n_estimators': [55],\n",
    "              'max_features': [5, 7, 9],\n",
    "              'max_depth'   : [None, 20],\n",
    "              'criterion'   : ['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "# Búsqueda por grid search con validación cruzada\n",
    "# ==============================================================================\n",
    "grid = GridSearchCV(\n",
    "        estimator  = ExtraTreesClassifier(random_state = 1995),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'f1_micro',\n",
    "        n_jobs     = multiprocessing.cpu_count() - 1,\n",
    "        cv         = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1995), \n",
    "        refit      = True,\n",
    "        verbose    = 1,\n",
    "        return_train_score = True\n",
    "       )\n",
    "\n",
    "grid.fit(X = X_train, y = y_train)\n",
    "\n",
    "# Resultados\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "resultados.filter(regex = '(param*|mean_t|std_t)') \\\n",
    "    .drop(columns = 'params') \\\n",
    "    .sort_values('f1_micro', ascending = False) \\\n",
    "    .head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Featuring Engineering and selection (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second models: Evaluating and comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Choosing best model, version management and reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Preparing for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Deploying to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8: Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Treveil, M. & the Dataiku Team. (2020). Introducing MLOps (1.a ed., Vol. 1). O’Reilly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Hunter, J., Dale, D., Firing, E., Droettboom, M., & The Matplotlib development team. (2002-2021). Matplotlib. Matplotlib 3.5.1 documentation. \n",
    "    https://matplotlib.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    scikit-learn: machine learning in Python — scikit-learn 1.0.2 documentation. (2021). Scikit-Learn. \n",
    "    https://scikit-learn.org/stable/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Wikipedia contributors. (2021, 12 septiembre). Out-of-bag error. Wikipedia. https://en.wikipedia.org/wiki/Out-of-bag_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (2019). Richter’s Predictor: Modeling Earthquake Damage. DrivenData. https://www.drivendata.org/competitions/57/nepal-earthquake/page/136/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Wikipedia contributors. (2022, 16 enero). Random forest. Wikipedia. https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Cross-validation: evaluating estimator performance. (2021). Scikit-Learn. https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de072921dc87486613898b1ef56959cc98c50a630fb49de1898fb32d92a683cf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
